<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Relational Data Streams in Go</title>
 <link href="http://jonlawlor.github.io/atom.xml" rel="self"/>
 <link href="http://jonlawlor.github.io/"/>
 <updated>2014-07-22T22:55:08-04:00</updated>
 <id>http://jonlawlor.github.io</id>
 <author>
   <name>Jonathan J Lawlor</name>
   <email>jonathan.lawlor@gmail.com</email>
 </author>

 
 <entry>
   <title>Group</title>
   <link href="http://jonlawlor.github.io/2014/07/22/group/"/>
   <updated>2014-07-22T00:00:00-04:00</updated>
   <id>http://jonlawlor.github.io/2014/07/22/group</id>
   <content type="html">&lt;p&gt;Group is a non-relational primative that I will be using to construct relational operations.  The basic idea is to put similar tuples together in a channel, and to delimit when a new similar group is being sent by putting it into another channel.&lt;/p&gt;

&lt;p&gt;As an example, if we use the tuple type from before:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;go&quot;&gt;&lt;span class=&quot;kd&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;foo&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;bar&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then given an example channel which produces tuples:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;go&quot;&gt;&lt;span class=&quot;nx&quot;&gt;ch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;chan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;ch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;ch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;ch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;ch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;ch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then we want the &lt;code&gt;group&lt;/code&gt; function to produce a channel that returns a channel that produces {1,1}, {1,2}, {1,3}, and then closes, and then produce the values {2,1}, {2,2} on another channel, and then closes that one.  Both of those channels will be returned in a channel, which can be thought of as a channel of groups.&lt;/p&gt;

&lt;p&gt;The requirements for a group function are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Takes input tuples from a channel with tuples that can be compared for equality.  Typically this will mean that some of the fields in a structure are equal, although it could also be values in a range, or values that are close to each other.&lt;/li&gt;
&lt;li&gt;Sends the result tuples on a channel of channels of tuples.  The inner channels each contain the similar group of tuples, and the outer channel represents the channel of all groups of tuples.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the face of it, we have a big problem if we try to perform this on an unordered input.  When the input is unordered, the last input tuple could belong to the same group as the first input tuple, which means that we can&amp;#39;t close the first group channel before the last input tuple has been examined.  With that in mind, we&amp;#39;ll implement the ordered version first.&lt;/p&gt;

&lt;h3&gt;Ordered Implementation&lt;/h3&gt;

&lt;p&gt;For a predicate, I typically use&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;go&quot;&gt;&lt;span class=&quot;kd&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;predicate&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;foo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;foo&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;script src=&quot;https://gist.github.com/jonlawlor/9d2ef31380685339693a.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;This function is a bit hairier than &lt;a href=&quot;/2014/07/18/distinct/&quot;&gt;distinct&lt;/a&gt;, but the 
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;go&quot;&gt;&lt;span class=&quot;kd&quot;&gt;chan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;chan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
 is actually a great fit for the problem.  We also could have made the buffering adaptive, and had it determine the length of the buffer based on the number of tuples in previous groups.  Also, note that the output has the same ordering as the input.&lt;/p&gt;

&lt;h3&gt;Ordered Performance&lt;/h3&gt;

&lt;p&gt;The following table compares the amount of time it takes to perform the first &lt;code&gt;group&lt;/code&gt; implementation on varying sizes of inputs.  Because the performance depends on both the number of input tuples and the number of unique tuples, I tried a variety of different combinations.  The leftmost column gives the number of tuples sent to the distinct function, and the other columns measure how long it took to perform distinct, given that it has (as many as, not exactly) some number of unique tuples.  All performance measurements are done on a Macbook with a 2.13 GHz Intel Core 2 Duo and 4 GB of RAM.  Times are in ns / operation.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr align=&quot;right&quot;&gt;
      &lt;th&gt;Tuples&lt;/th&gt;
      &lt;th&gt;3 Unique&lt;/th&gt;
      &lt;th&gt;1,000 Unique&lt;/th&gt;
      &lt;th&gt;100,000 Unique&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10&lt;/td&gt; &lt;td&gt;20,323&lt;/td&gt; &lt;td&gt;53,033&lt;/td&gt; &lt;td&gt;54,202&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100&lt;/td&gt;    &lt;td&gt;157,572&lt;/td&gt;    &lt;td&gt;317,374&lt;/td&gt;    &lt;td&gt;285,858&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000&lt;/td&gt;  &lt;td&gt;1,455,730&lt;/td&gt;  &lt;td&gt;2,093,007&lt;/td&gt;  &lt;td&gt;2,095,497&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10,000&lt;/td&gt; &lt;td&gt;14,780,515&lt;/td&gt; &lt;td&gt;20,130,534&lt;/td&gt; &lt;td&gt;21,011,306&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100,000&lt;/td&gt;    &lt;td&gt;151,403,277&lt;/td&gt;    &lt;td&gt;191,331,425&lt;/td&gt;    &lt;td&gt;220,990,957&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000,000&lt;/td&gt;  &lt;td&gt;1,524,802,692&lt;/td&gt;  &lt;td&gt;1,741,569,855&lt;/td&gt;  &lt;td&gt;1,756,899,742&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If you divide through by the number of tuples, you get the following table (units are in ns/tuple)&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr align=&quot;right&quot;&gt;
      &lt;th&gt;Tuples&lt;/th&gt;
      &lt;th&gt;3 Unique&lt;/th&gt;
      &lt;th&gt;1,000 Unique&lt;/th&gt;
      &lt;th&gt;100,000 Unique&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10&lt;/td&gt; &lt;td&gt;2,032&lt;/td&gt;  &lt;td&gt;5,303&lt;/td&gt;  &lt;td&gt;5,420&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100&lt;/td&gt;    &lt;td&gt;1,576&lt;/td&gt;  &lt;td&gt;3,174&lt;/td&gt;  &lt;td&gt;2,859&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000&lt;/td&gt;  &lt;td&gt;1,456&lt;/td&gt;  &lt;td&gt;2,093&lt;/td&gt;  &lt;td&gt;2,095&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10,000&lt;/td&gt; &lt;td&gt;1,478&lt;/td&gt;  &lt;td&gt;2,013&lt;/td&gt;  &lt;td&gt;2,101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100,000&lt;/td&gt;    &lt;td&gt;1,514&lt;/td&gt;  &lt;td&gt;1,913&lt;/td&gt;  &lt;td&gt;2,210&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000,000&lt;/td&gt;  &lt;td&gt;1,525&lt;/td&gt;  &lt;td&gt;1,742&lt;/td&gt;  &lt;td&gt;1,757&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Other than a small startup cost, it seems to stabilize around 1,750 ns/tuple, which is a bit more than 500k tuples/second.  The results are in the same order as the input, as well.&lt;/p&gt;

&lt;h3&gt;Unordered Implementation&lt;/h3&gt;

&lt;p&gt;Now to implement the unordered version.  It can&amp;#39;t perform the same kind of arbitrary predicate evaluation as the ordered version.  &lt;/p&gt;

&lt;p&gt;The most natural way to represent the groups is to have each group in a map, where the key is a subdomain of the input tuple, and the values of the map are slices of tuples that belong to a group.  We can start producing values on the first group immediately, but every group after that has to wait for the input to close, which can cause significant latency if it blocks.&lt;/p&gt;

&lt;p&gt;Also, basically all of the tuples will be in memory, so for large data sets, this implementation will need some more work.  The results will be unordered, but it will be able to produce initial values quickly, which will be useful later.&lt;/p&gt;

&lt;p&gt;For testing, I use something like this:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;go&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// we can only perform grouping on equality when we&amp;#39;re using a hash&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;subTup&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;struct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;foo&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;proj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;subTup&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;subTup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;script src=&quot;https://gist.github.com/jonlawlor/bdd38e1448a261095706.js&quot;&gt; &lt;/script&gt;

&lt;h3&gt;Performance&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr align=&quot;right&quot;&gt;
      &lt;th&gt;Tuples&lt;/th&gt;
      &lt;th&gt;3 Unique&lt;/th&gt;
      &lt;th&gt;1,000 Unique&lt;/th&gt;
      &lt;th&gt;100,000 Unique&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10&lt;/td&gt; &lt;td&gt;21,071&lt;/td&gt; &lt;td&gt;26,271&lt;/td&gt; &lt;td&gt;27,268&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100&lt;/td&gt;    &lt;td&gt;146,088&lt;/td&gt;    &lt;td&gt;232,951&lt;/td&gt;    &lt;td&gt;243,976&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000&lt;/td&gt;  &lt;td&gt;1,369,260&lt;/td&gt;  &lt;td&gt;2,078,356&lt;/td&gt;  &lt;td&gt;2,534,798&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10,000&lt;/td&gt; &lt;td&gt;13,725,700&lt;/td&gt; &lt;td&gt;16,817,520&lt;/td&gt; &lt;td&gt;25,318,781&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100,000&lt;/td&gt;    &lt;td&gt;135,798,571&lt;/td&gt;    &lt;td&gt;159,113,453&lt;/td&gt;    &lt;td&gt;238,822,794&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000,000&lt;/td&gt;  &lt;td&gt;1,373,282,652&lt;/td&gt;  &lt;td&gt;1,536,476,473&lt;/td&gt;  &lt;td&gt;1,835,299,731&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So the performance is basically the same or better than the ordered version (~500k tuples/second), despite having to fully populate the map of groups to tuples before sending the data from the second group onwards.  I find that amazing and I had expected it to be much worse, because each tuple has more comparisons this way, and the slices have to append many times.  Of course, in testing I never blocked the input, which would most likely occur in real world situations, so it is much more sensitive to input performance.&lt;/p&gt;

&lt;h3&gt;Parallel Unordered Implementation&lt;/h3&gt;

&lt;p&gt;We can use the same striping method used in the &lt;a href=&quot;/2014/07/18/distinct/&quot;&gt;distinct&lt;/a&gt; parallel implementation to parallelize the grouping operation.  Basically, each tuple goes through a call to modulo to determine which grouping goroutine it should be sent to, and everything else is basically the same.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/jonlawlor/ca8a37edae9926e1db55.js&quot;&gt; &lt;/script&gt;

&lt;h3&gt;Parallel Unordered Performance&lt;/h3&gt;

&lt;p&gt;I ran the parallel implementation with GOMAXPROCS set to 2:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr align=&quot;right&quot;&gt;
      &lt;th&gt;Tuples&lt;/th&gt;
      &lt;th&gt;3 Unique&lt;/th&gt;
      &lt;th&gt;1,000 Unique&lt;/th&gt;
      &lt;th&gt;100,000 Unique&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10&lt;/td&gt; &lt;td&gt;50,180&lt;/td&gt; &lt;td&gt;78,559&lt;/td&gt; &lt;td&gt;101,311&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100&lt;/td&gt;    &lt;td&gt;259,308&lt;/td&gt;    &lt;td&gt;445,692&lt;/td&gt;    &lt;td&gt;480,127&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000&lt;/td&gt;  &lt;td&gt;2,111,440&lt;/td&gt;  &lt;td&gt;3,572,496&lt;/td&gt;  &lt;td&gt;4,134,282&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10,000&lt;/td&gt; &lt;td&gt;23,890,105&lt;/td&gt; &lt;td&gt;32,874,142&lt;/td&gt; &lt;td&gt;38,772,126&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100,000&lt;/td&gt;    &lt;td&gt;289,065,745&lt;/td&gt;    &lt;td&gt;272,000,854&lt;/td&gt;    &lt;td&gt;322,471,274&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000,000&lt;/td&gt;  &lt;td&gt;3,018,390,109&lt;/td&gt;  &lt;td&gt;2,748,059,524&lt;/td&gt;  &lt;td&gt;3,806,559,013&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This performance is significantly worse than either of the two other versions, which probably means that context switching is killing the performance (~250k tuples/sec).  It is possible that with some hardware and tuple types this would outperform the unordered version, though.  If anyone has a more performant implementation I&amp;#39;d love to revisit this code, because it will be central to some later relational operations.  I&amp;#39;ve got to say I like how the same slicing idiom works in unordered approaches.&lt;/p&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;These grouping operations aren&amp;#39;t quite as fast as distinct, but they are still quite fast.  I&amp;#39;m surprised by how fast the unordered group operation is, and somewhat less surprised that striping didn&amp;#39;t help matters.  There is still some work to be done - in particular, the groups should be cancel-able for them to be really useful, which is something I&amp;#39;m going to add in a later post.  Also, if you have a better implementation I would be happy to update this post or add it to a later one, as appropriate!&lt;/p&gt;

&lt;h3&gt;Thanks&lt;/h3&gt;

&lt;p&gt;I&amp;#39;m happy to thank mb0 and brentmn on #go-nuts.  mb0 wrote a much more appropriate data structure I was using for the unordered grouping operation, and the result was far superior speed and lower memory usage.  If you are morbidly curious, I had originally implemented the grouping with a goroutine for each of the groups, that was live until the input closed.  When large amounts of unique inputs were set to that version, the benchmarks couldn&amp;#39;t even complete in some cases!  brentmn pointed out a race condition as well, although I didn&amp;#39;t track it down and instead just re-implemented the whole thing.  So lesson learned: first implement without goroutines or channels.  Boil down your algorithm to the simplest thing you can, and add complications later.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Distinct</title>
   <link href="http://jonlawlor.github.io/2014/07/18/distinct/"/>
   <updated>2014-07-18T00:00:00-04:00</updated>
   <id>http://jonlawlor.github.io/2014/07/18/distinct</id>
   <content type="html">&lt;p&gt;The first operations we look at aren&amp;#39;t even relational, but are useful for building other relational operations.  Distinct isn&amp;#39;t a relational operation (although it is included in SQL) because by definition, relations are sets, and sets are always unique.  However, it is extremely useful, and represents a good starting point.  It also raises some interesting tradeoffs.&lt;/p&gt;

&lt;p&gt;The requirements for a distinct function are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Takes input tuples from a channel with (possibly) duplicate tuples&lt;/li&gt;
&lt;li&gt;Sends results to an output channel with no duplicate tuples&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In Go, its signature should look something like this:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;go&quot;&gt;&lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;distinct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;chan&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;chan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For testing, I usually use a placeholder type fooBar in place of tuple:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;go&quot;&gt;&lt;span class=&quot;kd&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;fooBar&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;foo&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;bar&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Simple Implementation&lt;/h3&gt;

&lt;script src=&quot;https://gist.github.com/jonlawlor/7e6b351f9df6338527d7.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;There are a few things to notice about that distinct function.  When the inner, anonymous function terminates the &lt;code&gt;mem[tuple]struct{}{}&lt;/code&gt; will have every tuple that it has ever sent still in it, which could be a problem.  Also, it can&amp;#39;t be run in parallel between the same input and results channels, because then there would be two maps, and they would race to get and send values, which would cause duplicates.  On the plus side, it is quite succinct.&lt;/p&gt;

&lt;h3&gt;Simple Performance&lt;/h3&gt;

&lt;p&gt;I&amp;#39;ve implemented some simple test helping functions to generate example channels of tuples, that are used for benchmarking.  The code is found in my &lt;a href=&quot;https://github.com/jonlawlor/relpipes&quot;&gt;relpipes&lt;/a&gt; package.  The computer used for testing is a Macbook with a 2.13 GHz Intel Core 2 Duo and 4 GB of RAM.&lt;/p&gt;

&lt;p&gt;The following table compares the amount of time it takes to perform the first &lt;code&gt;distinct&lt;/code&gt; implementation on varying sizes of inputs.  Because the performance depends on both the number of input tuples and the number of unique tuples, I tried a variety of different combinations.  The leftmost column gives the number of tuples sent to the distinct function, and the other columns measure how long it took to perform distinct, given that it has (as many as, not exactly) some number of unique tuples.  All times are in ns / operation.&lt;/p&gt;

&lt;table&gt;
    &lt;thead&gt;
        &lt;tr align=&quot;right&quot;&gt;
            &lt;th&gt;Tuples&lt;/th&gt;
            &lt;th&gt;3 Unique&lt;/th&gt;
            &lt;th&gt;1,000 Unique&lt;/th&gt;
            &lt;th&gt;100,000 Unique&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
&lt;tr align=&quot;right&quot;&gt;
    &lt;td&gt;10&lt;/td&gt; &lt;td&gt;11,418&lt;/td&gt; &lt;td&gt;16,639&lt;/td&gt; &lt;td&gt;16,669&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
    &lt;td&gt;100&lt;/td&gt;    &lt;td&gt;76,272&lt;/td&gt; &lt;td&gt;147,966&lt;/td&gt;    &lt;td&gt;149,888&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
    &lt;td&gt;1000&lt;/td&gt;   &lt;td&gt;717,874&lt;/td&gt;    &lt;td&gt;1,268,456&lt;/td&gt;  &lt;td&gt;1,579,900&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
    &lt;td&gt;10,000&lt;/td&gt; &lt;td&gt;7,211,543&lt;/td&gt;  &lt;td&gt;8,190,355&lt;/td&gt;  &lt;td&gt;15,097,732&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
    &lt;td&gt;100,000&lt;/td&gt;    &lt;td&gt;71,962,786&lt;/td&gt; &lt;td&gt;73,765,912&lt;/td&gt; &lt;td&gt;130,224,777&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
    &lt;td&gt;1,000,000&lt;/td&gt;  &lt;td&gt;724,689,732&lt;/td&gt;    &lt;td&gt;730,728,605&lt;/td&gt;    &lt;td&gt;854,284,016&lt;/td&gt;
&lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As you can see, the time it takes grows proportionally to the number of input tuples, and depending on the number of unique values, there is an additional overhead.  I believe that is because the implementation hashes twice when a new tuple is added to the map, while a duplicate value only has a single comparison to make.&lt;/p&gt;

&lt;p&gt;In this next table, I&amp;#39;ve divided the ns/op by the number of tuples, to get a sense for the per-tuple overhead.  Otherwise the rows and columns have the same meaning.  Units are ns/tuple input.
&lt;table&gt;
    &lt;thead&gt;
        &lt;tr align=&quot;right&quot;&gt;
            &lt;th&gt;Tuples&lt;/th&gt;
            &lt;th&gt;3 Unique&lt;/th&gt;
            &lt;th&gt;1,000 Unique&lt;/th&gt;
            &lt;th&gt;100,000 Unique&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10&lt;/td&gt; &lt;td&gt;1,142&lt;/td&gt; &lt;td&gt;1,664&lt;/td&gt; &lt;td&gt;1,667&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100&lt;/td&gt; &lt;td&gt;763&lt;/td&gt; &lt;td&gt;1,480&lt;/td&gt; &lt;td&gt;1,499&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000&lt;/td&gt; &lt;td&gt;718&lt;/td&gt; &lt;td&gt;1,268&lt;/td&gt; &lt;td&gt;1,580&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10,000&lt;/td&gt; &lt;td&gt;721&lt;/td&gt; &lt;td&gt;819&lt;/td&gt; &lt;td&gt;1,510&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100,000&lt;/td&gt; &lt;td&gt;720&lt;/td&gt; &lt;td&gt;738&lt;/td&gt; &lt;td&gt;1,302&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000,000&lt;/td&gt; &lt;td&gt;725&lt;/td&gt; &lt;td&gt;731&lt;/td&gt; &lt;td&gt;854&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;It looks like the implementation takes about twice as long (1600 ns / tuple) when tuples are new, and about 800 ns / tuple otherwise.  This translates to somewhere in the range of 600k - 1.2M tuples / second.  If the channel which sends tuples to this function produced more than that rate, this function would fall behind.  There are some methods to deal with that (beyond faster hardware &amp;amp; better performance) that we will get into in a later blog post.&lt;/p&gt;

&lt;h3&gt;Parallel Implementation&lt;/h3&gt;

&lt;p&gt;So let&amp;#39;s take a stab at parallelism.  One response might be to just run the inner goroutine in parallel, but it will share a map, and &lt;a href=&quot;http://golang.org/doc/faq#atomic_maps&quot;&gt;maps are not concurrent&lt;/a&gt;.  We can fix that with a &lt;a href=&quot;http://golang.org/pkg/sync/#Mutex&quot;&gt;sync.Mutex&lt;/a&gt; to the mem.  In addition, if we have several goroutines handling the result channel, they have to coordinate closing that channel.  We can fix that with a &lt;a href=&quot;http://golang.org/pkg/sync/#WaitGroup&quot;&gt;sync.WaitGroup&lt;/a&gt; and &lt;em&gt;another&lt;/em&gt; goroutine.  In this case I&amp;#39;ve decided to include an input argument &lt;code&gt;n&lt;/code&gt; which tells it how many goroutines should operate concurrently.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/jonlawlor/c397499881da24fbf49d.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Now, when the input n to the parallel distinct function is &amp;gt; 1, there will be several goroutines all pulling tuples from the input, and sending them to the output.  However, it seems like the process doesn&amp;#39;t gain much because the map has to be shared.&lt;/p&gt;

&lt;h3&gt;Parallel Performance&lt;/h3&gt;

&lt;p&gt;So, how does the parallel version perform?  I set GOMAXPROCs to 2, and performed the same tests as above (units are ns/tuple input):&lt;/p&gt;

&lt;table&gt;
    &lt;thead&gt;
        &lt;tr align=&quot;right&quot;&gt;
            &lt;th&gt;Tuples&lt;/th&gt;
            &lt;th&gt;3 Unique&lt;/th&gt;
            &lt;th&gt;1,000 Unique&lt;/th&gt;
            &lt;th&gt;100,000 Unique&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10&lt;/td&gt; &lt;td&gt;28,669&lt;/td&gt; &lt;td&gt;33,234&lt;/td&gt; &lt;td&gt;33,060&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100&lt;/td&gt;    &lt;td&gt;215,048&lt;/td&gt;    &lt;td&gt;212,194&lt;/td&gt;    &lt;td&gt;222,439&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000&lt;/td&gt;  &lt;td&gt;2,137,649&lt;/td&gt;  &lt;td&gt;2,259,830&lt;/td&gt;  &lt;td&gt;1,857,265&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10,000&lt;/td&gt; &lt;td&gt;21,900,233&lt;/td&gt; &lt;td&gt;21,814,935&lt;/td&gt; &lt;td&gt;17,800,176&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100,000&lt;/td&gt;    &lt;td&gt;177,486,647&lt;/td&gt;    &lt;td&gt;179,204,410&lt;/td&gt;    &lt;td&gt;233,804,969&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000,000&lt;/td&gt;  &lt;td&gt;1,713,882,948&lt;/td&gt;  &lt;td&gt;1,830,978,407&lt;/td&gt;  &lt;td&gt;2,449,220,450&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Well, crap.  It is slower, by about a factor of 2.5x!  The fact that performance has degraded is a very strong suggestion that the locking is adding more overhead, without a corresponding performance boost.  Not all problems benefit from parallelism.  I&amp;#39;m not going to bother showing the per-tuple performance.  On a side note, setting GOMAXPROCS to 2 is actually slightly slower than the default of 1.&lt;/p&gt;

&lt;h3&gt;Parallel Implementation, Revised&lt;/h3&gt;

&lt;p&gt;Instead of managing concurrent access to a map, we can split up the input to use different maps depending on the attributes of the input tuples, by using a fast modulo operator.&lt;/p&gt;

&lt;p&gt;For tests, I used:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;go&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// Mod takes an input tuple and returns an int in&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// [0, n).  Ideally it will be uniformly distributed.&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;foo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;n&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To modulo using a string, you may want to use &lt;a href=&quot;http://golang.org/pkg/hash/crc32/&quot;&gt;hash/crc32&lt;/a&gt; first, which is implemented in hardware on &lt;a href=&quot;http://golang.org/src/pkg/hash/crc32/crc32_amd64x.go&quot;&gt;amd64 architecture&lt;/a&gt;.  I haven&amp;#39;t tested its performance, though.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/jonlawlor/7da394434eb7240710b6.js&quot;&gt; &lt;/script&gt;

&lt;h3&gt;Revised Parallel Performance&lt;/h3&gt;

&lt;table&gt;
    &lt;thead&gt;
        &lt;tr align=&quot;right&quot;&gt;
            &lt;th&gt;Tuples&lt;/th&gt;
            &lt;th&gt;3 Unique&lt;/th&gt;
            &lt;th&gt;1,000 Unique&lt;/th&gt;
            &lt;th&gt;100,000 Unique&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10&lt;/td&gt; &lt;td&gt;30,443&lt;/td&gt; &lt;td&gt;35,229&lt;/td&gt; &lt;td&gt;30,068&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100&lt;/td&gt;    &lt;td&gt;169,240&lt;/td&gt;    &lt;td&gt;208,973&lt;/td&gt;    &lt;td&gt;196,924&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000&lt;/td&gt;  &lt;td&gt;1,442,566&lt;/td&gt;  &lt;td&gt;1,466,887&lt;/td&gt;  &lt;td&gt;1,624,803&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10,000&lt;/td&gt; &lt;td&gt;14,243,882&lt;/td&gt; &lt;td&gt;14,492,360&lt;/td&gt; &lt;td&gt;21,312,597&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100,000&lt;/td&gt;    &lt;td&gt;143,051,464&lt;/td&gt;    &lt;td&gt;141,408,320&lt;/td&gt;    &lt;td&gt;192,644,214&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000,000&lt;/td&gt;  &lt;td&gt;1,425,506,632&lt;/td&gt;  &lt;td&gt;1,394,233,550&lt;/td&gt;  &lt;td&gt;1,447,067,496&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The revised implementation has better performance than the original one, but it still isn&amp;#39;t as fast as the non-concurrent version.  It might scale better with different hardware and different tuple types though.&lt;/p&gt;

&lt;h3&gt;Ordered Implementation&lt;/h3&gt;

&lt;p&gt;If we are willing to make some restrictions on how the input channel&amp;#39;s data is ordered, then we can avoid using the map altogether.  Instead, we can just compare each input tuple with the previously sent result tuple, and if it is equal to it, then we discard it.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/jonlawlor/b490a04d363a6f04276c.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;This has a big advantage over the first two in terms of memory use.  Also, it returns an ordered output.  However, it can&amp;#39;t be used for unordered inputs.  Knowing the characteristics of the input makes a big difference in terms of memory, and as we will see, it also makes it faster.&lt;/p&gt;

&lt;h3&gt;Ordered Performance&lt;/h3&gt;

&lt;p&gt;I haven&amp;#39;t benchmarked the memory usage of the previous two, but by inspection we can see that the memory usage will grow with the number of unique tuples, without bound.  The ordered program has constant memory usage.  Here is its performance (ns/op):&lt;/p&gt;

&lt;table&gt;
    &lt;thead&gt;
        &lt;tr align=&quot;right&quot;&gt;
            &lt;th&gt;Tuples&lt;/th&gt;
            &lt;th&gt;3 Unique&lt;/th&gt;
            &lt;th&gt;1,000 Unique&lt;/th&gt;
            &lt;th&gt;100,000 Unique&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10&lt;/td&gt; &lt;td&gt;11,941&lt;/td&gt; &lt;td&gt;13,041&lt;/td&gt; &lt;td&gt;12,791&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100&lt;/td&gt;    &lt;td&gt;89,945&lt;/td&gt; &lt;td&gt;101,685&lt;/td&gt;    &lt;td&gt;101,925&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000&lt;/td&gt;  &lt;td&gt;908,702&lt;/td&gt;    &lt;td&gt;996,094&lt;/td&gt;    &lt;td&gt;995,019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10,000&lt;/td&gt; &lt;td&gt;8,780,944&lt;/td&gt;  &lt;td&gt;10,039,774&lt;/td&gt; &lt;td&gt;10,078,654&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100,000&lt;/td&gt;    &lt;td&gt;88,551,917&lt;/td&gt; &lt;td&gt;101,807,795&lt;/td&gt;    &lt;td&gt;101,829,774&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000,000&lt;/td&gt;  &lt;td&gt;884,714,522&lt;/td&gt;    &lt;td&gt;1,022,781,810&lt;/td&gt;  &lt;td&gt;1,015,324,417&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Well, this is slightly slower than the unordered version (~20% compared to best) but while the unordered version is still building up its map, this version is faster.  As an added bonus, the values coming out will be in the same order as those coming in.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s the same table in ns/tuple:&lt;/p&gt;

&lt;table&gt;
    &lt;thead&gt;
        &lt;tr align=&quot;right&quot;&gt;
            &lt;th&gt;Tuples&lt;/th&gt;
            &lt;th&gt;3 Unique&lt;/th&gt;
            &lt;th&gt;1,000 Unique&lt;/th&gt;
            &lt;th&gt;100,000 Unique&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10&lt;/td&gt; &lt;td&gt;1,194&lt;/td&gt;  &lt;td&gt;1,304&lt;/td&gt;  &lt;td&gt;1,279&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100&lt;/td&gt;    &lt;td&gt;899&lt;/td&gt;    &lt;td&gt;1,017&lt;/td&gt;  &lt;td&gt;1,019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000&lt;/td&gt;  &lt;td&gt;909&lt;/td&gt;    &lt;td&gt;996&lt;/td&gt;    &lt;td&gt;995&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;10,000&lt;/td&gt; &lt;td&gt;878&lt;/td&gt;    &lt;td&gt;1,004&lt;/td&gt;  &lt;td&gt;1,008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;100,000&lt;/td&gt;    &lt;td&gt;886&lt;/td&gt;    &lt;td&gt;1,018&lt;/td&gt;  &lt;td&gt;1,018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&quot;right&quot;&gt;
&lt;td&gt;1,000,000&lt;/td&gt;  &lt;td&gt;885&lt;/td&gt;    &lt;td&gt;1,023&lt;/td&gt;  &lt;td&gt;1,015&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So long term, this seems pretty stable around 1M tuples/s.  I&amp;#39;m not sure why there is a jump from 3 unique values to 1000 unique values, so if anyone has any ideas on that I&amp;#39;d like to hear them.&lt;/p&gt;

&lt;p&gt;On the other hand, the ordered distinct requires that the input is sorted.  If it is not already sorted, then we either have to use a different algorithm, or we have to read the entire (possibly &lt;em&gt;enormous&lt;/em&gt;) input channel, and sort the whole thing before we can start producing results, which will result in enormous latency.&lt;/p&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;Even something as simple as finding unique values in an input channel exhibits some interesting tradeoffs, and we&amp;#39;ve barely scratched the surface of data streams.  As a general rule: unsorted data streams often (but not always) require an unbounded amount of memory for exact results.  If we can impose some restrictions on how tuples will be input to a data stream operation, then we can reduce the amount of memory used dramatically, although that doesn&amp;#39;t always mean better performance.  And adding more cores doesn&amp;#39;t always result in better performance.&lt;/p&gt;

&lt;p&gt;We&amp;#39;ll probably be revisiting these algorithms later, using the unsafe package.  If you have suggestions as to better implementations (or implementations that are better in some situations), I&amp;#39;ll try to either update this post, or add them to the followup.&lt;/p&gt;

&lt;h3&gt;Thanks&lt;/h3&gt;

&lt;p&gt;Egon Elbre helped review and provided a revised version of the unordered distinct operation.  He also gave an implementation using the unsafe package, which was too advanced for this post.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>When and Why to use Relational Datastreams</title>
   <link href="http://jonlawlor.github.io/2014/07/17/why-relational-datastreams/"/>
   <updated>2014-07-17T00:00:00-04:00</updated>
   <id>http://jonlawlor.github.io/2014/07/17/why-relational-datastreams</id>
   <content type="html">&lt;p&gt;Pipelines in go are a &lt;a href=&quot;http://blog.golang.org/pipelines&quot;&gt;powerful idiom&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Relational_model&quot;&gt;relational data model&lt;/a&gt; is an extremely effective and succinct way of manipulating large quantities of data.  It is also extremely popular: it puts the &lt;strong&gt;R&lt;/strong&gt; in RDBMS and ORM.&lt;/p&gt;

&lt;p&gt;Combining the two yields a variety of higher level composable &amp;quot;chunks&amp;quot; that can be used to reliably solve problems.  Depending on how they are constructed, the pipeline stages can be tuned to different performance characteristics.&lt;/p&gt;

&lt;p&gt;This approach (and relational algebra) is amenable to two forms of concurrency: pipelining, where results from one query are piped to another as soon as they are available, and parallelism, where many records / tuples are being processed simultaneously.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why have parallel database systems become more than a research curiosity?  One explanation is the widespread adoption of the relational data model.  In 1983 relational database systems were just appearing in the marketplace; today they dominate it.  Relational queries are ideally suited to parallel execution; they consist of uniform operations applied to uniform streams of data.  Each operator produces a new relation, so the operators can be composed into highly parallel dataflow graphs.  By streaming the output of one operator into the input of another operator, the two operators can work in series giving pipelined parallelism.  By partitioning the input data among multiple processors and memories, an operator can often be split into many independent operators each working on a part of the data.  This partitioned data and execution gives pipelined parallelism.&lt;/p&gt;

&lt;p&gt;-- &lt;cite&gt;DeWitt, David, and Jim Gray. &amp;quot;Parallel database systems: the future of high performance database systems.&amp;quot; Communications of the ACM 35.6 (1992): 85-98.&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Go&amp;#39;s pipeline constructs match up very well, and can be applied to construct continuous queries which produce new results as soon as new data is available.&lt;/p&gt;

&lt;p&gt;In this blog, relational operations are represented as functions which take one or more channels of input tuples or records, and typically return one or more channels which produce output tuples.&lt;/p&gt;

&lt;p&gt;That representation brings with it some interesting questions, which correspond to problems in data streams.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. ... Examples of such applications include financial applications, network monitoring, security, telecommunications data management, web applications, manufacturing, sensor networks, and others.&lt;/p&gt;

&lt;p&gt;--&lt;cite&gt;Babcock, Brian, et al. &amp;quot;Models and issues in data stream systems.&amp;quot; Proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems. ACM, 2002.&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You may find that you have already been using these techniques in some of your code; in that case, these articles may provide a broader context.  None of them are intended to be a one size fits all solution.  You&amp;#39;ll have to adapt particular implementations of the operations to your needs.  &lt;/p&gt;

&lt;p&gt;If you aren&amp;#39;t familiar with relational algebra, see &lt;a href=&quot;http://books.google.com/books?id=TR8f5dtnC9IC&amp;amp;lpg=PP1&amp;amp;dq=isbn%3A0596100124&amp;amp;pg=PP1#v=onepage&amp;amp;q&amp;amp;f=false&quot;&gt;Database in Depth&lt;/a&gt; by C. J. Date for an excellent introduction.  I try to use the same terminology as that book.&lt;/p&gt;

&lt;p&gt;Of course, not all problems can be solved in this way, and not all of the ones that can be solved this way should be.  Data stream algorithms are often concerned with how to deal with very large (potentially unending) amounts of data that might be arriving too fast for consumption.  This gives rise to &lt;em&gt;sub-linear&lt;/em&gt; algorithms, which provide approximate results instead of exact ones.  If you need an exact answer, or don&amp;#39;t have to deal with very large data sets, these algorithms may not be appropriate.&lt;/p&gt;
</content>
 </entry>
 

</feed>
