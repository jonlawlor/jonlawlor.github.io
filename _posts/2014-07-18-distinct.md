---
layout: post
title: Distinct
---

The first operations we look at aren't even relational, but are useful for building other relational operations.  Distinct isn't a relational operation (although it is included in SQL) because by definition, relations are sets, and sets are always unique.  However, it is extremely useful, and represents a good starting point.  It also raises some interesting tradeoffs.

The requirements for a distinct function are as follows:

* Takes input tuples from a channel with (possibly) duplicate tuples
* Sends results to an output channel with no duplicate tuples

In Go, its signature should look something like this:
{% highlight go %}
func distinct(in <-chan tuple, res chan<- tuple)
{% endhighlight %}

For testing, I usually use a placeholder type fooBar in place of tuple:
{% highlight go %}
type fooBar struct {
	foo int
	bar int
}
{% endhighlight %}

### Simple Implementation

{% gist jonlawlor/7e6b351f9df6338527d7 %}

There are a few things to notice about that distinct function.  When the inner, anonymous function terminates the `mem[tuple]struct{}{}` will have every tuple that it has ever sent still in it, which could be a problem.  Also, it can't be run in parallel between the same input and results channels, because then there would be two maps, and they would race to get and send values, which would cause duplicates.  On the plus side, it is quite succinct.

### Simple Performance

I've implemented some simple test helping functions to generate example channels of tuples, that are used for benchmarking.  The code is found in my [relpipes](https://github.com/jonlawlor/relpipes) package.  The computer used for testing is a Macbook with a 2.13 GHz Intel Core 2 Duo and 4 GB of RAM.

The following table compares the amount of time it takes to perform the first `distinct` implementation on varying sizes of inputs.  Because the performance depends on both the number of input tuples and the number of unique tuples, I tried a variety of different combinations.  The leftmost column gives the number of tuples sent to the distinct function, and the other columns measure how long it took to perform distinct, given that it has (as many as, not exactly) some number of unique tuples.  All times are in ns / operation.

<table>
	<thead>
		<tr align="right">
			<th>Tuples</th>
			<th>3 Unique</th>
			<th>1,000 Unique</th>
			<th>100,000 Unique</th>
		</tr>
	</thead>
	<tbody>
<tr align="right">
	<td>10</td>	<td>11,418</td>	<td>16,639</td>	<td>16,669</td>
</tr>
<tr align="right">
	<td>100</td>	<td>76,272</td>	<td>147,966</td>	<td>149,888</td>
</tr>
<tr align="right">
	<td>1000</td>	<td>717,874</td>	<td>1,268,456</td>	<td>1,579,900</td>
</tr>
<tr align="right">
	<td>10,000</td>	<td>7,211,543</td>	<td>8,190,355</td>	<td>15,097,732</td>
</tr>
<tr align="right">
	<td>100,000</td>	<td>71,962,786</td>	<td>73,765,912</td>	<td>130,224,777</td>
</tr>
<tr align="right">
	<td>1,000,000</td>	<td>724,689,732</td>	<td>730,728,605</td>	<td>854,284,016</td>
</tr>
	</tbody>
</table>

As you can see, the time it takes grows proportionally to the number of input tuples, and depending on the number of unique values, there is an additional overhead.  I believe that is because the implementation hashes twice when a new tuple is added to the map, while a duplicate value only has a single comparison to make.


In this next table, I've divided the ns/op by the number of tuples, to get a sense for the per-tuple overhead.  Otherwise the rows and columns have the same meaning.  Units are ns/tuple input.
<table>
	<thead>
		<tr align="right">
			<th>Tuples</th>
			<th>3 Unique</th>
			<th>1,000 Unique</th>
			<th>100,000 Unique</th>
		</tr>
	</thead>
<tbody>
<tr align="right">
<td>10</td> <td>1,142</td> <td>1,664</td> <td>1,667</td>
</tr>
<tr align="right">
<td>100</td> <td>763</td> <td>1,480</td> <td>1,499</td>
</tr>
<tr align="right">
<td>1,000</td> <td>718</td> <td>1,268</td> <td>1,580</td>
</tr>
<tr align="right">
<td>10,000</td> <td>721</td> <td>819</td> <td>1,510</td>
</tr>
<tr align="right">
<td>100,000</td> <td>720</td> <td>738</td> <td>1,302</td>
</tr>
<tr align="right">
<td>1,000,000</td> <td>725</td> <td>731</td> <td>854</td>
</tr>
</tbody>
</table>

It looks like the implementation takes about twice as long (1600 ns / tuple) when tuples are new, and about 800 ns / tuple otherwise.  This translates to somewhere in the range of 600k - 1.2M tuples / second.  If the channel which sends tuples to this function produced more than that rate, this function would fall behind.  There are some methods to deal with that (beyond faster hardware & better performance) that we will get into in a later blog post.

### Parallel Implementation

So let's take a stab at parallelism.  One response might be to just run the inner goroutine in parallel, but it will share a map, and [maps are not concurrent](http://golang.org/doc/faq#atomic_maps).  We can fix that with a [sync.Mutex](http://golang.org/pkg/sync/#Mutex) to the mem.  In addition, if we have several goroutines handling the result channel, they have to coordinate closing that channel.  We can fix that with a [sync.WaitGroup](http://golang.org/pkg/sync/#WaitGroup) and *another* goroutine.  In this case I've decided to include an input argument `n` which tells it how many goroutines should operate concurrently.

{% gist jonlawlor/c397499881da24fbf49d %}

Now, when the input n to the parallel distinct function is > 1, there will be several goroutines all pulling tuples from the input, and sending them to the output.  However, it seems like the process doesn't gain much because the map has to be shared.

### Parallel Performance

So, how does the parallel version perform?  I set GOMAXPROCs to 2, and performed the same tests as above (units are ns/tuple input):

<table>
	<thead>
		<tr align="right">
			<th>Tuples</th>
			<th>3 Unique</th>
			<th>1,000 Unique</th>
			<th>100,000 Unique</th>
		</tr>
	</thead>
<tbody>
<tr align="right">
<td>10</td>	<td>28,669</td>	<td>33,234</td>	<td>33,060</td>
</tr>
<tr align="right">
<td>100</td>	<td>215,048</td>	<td>212,194</td>	<td>222,439</td>
</tr>
<tr align="right">
<td>1,000</td>	<td>2,137,649</td>	<td>2,259,830</td>	<td>1,857,265</td>
</tr>
<tr align="right">
<td>10,000</td>	<td>21,900,233</td>	<td>21,814,935</td>	<td>17,800,176</td>
</tr>
<tr align="right">
<td>100,000</td>	<td>177,486,647</td>	<td>179,204,410</td>	<td>233,804,969</td>
</tr>
<tr align="right">
<td>1,000,000</td>	<td>1,713,882,948</td>	<td>1,830,978,407</td>	<td>2,449,220,450</td>
</tr>
</tbody>
</table>

Well, crap.  It is slower, by about a factor of 2.5x!  The fact that performance has degraded is a very strong suggestion that the locking is adding more overhead, without a corresponding performance boost.  Not all problems benefit from parallelism.  I'm not going to bother showing the per-tuple performance.  On a side note, setting GOMAXPROCS to 2 is actually slightly slower than the default of 1.

### Parallel Implementation, Revised

Instead of managing concurrent access to a map, we can split up the input to use different maps depending on the attributes of the input tuples, by using a fast modulo operator.

For tests, I used:

{% highlight go %}
// Mod takes an input tuple and returns an int in
// [0, n).  Ideally it will be uniformly distributed.
func Mod(v tuple, n int) int {
	return v.foo % n
}
{% endhighlight %}


To modulo using a string, you may want to use [hash/crc32](http://golang.org/pkg/hash/crc32/) first, which is implemented in hardware on [amd64 architecture](http://golang.org/src/pkg/hash/crc32/crc32_amd64x.go).  I haven't tested its performance, though.

{% gist jonlawlor/7da394434eb7240710b6 %}

### Revised Parallel Performance

<table>
	<thead>
		<tr align="right">
			<th>Tuples</th>
			<th>3 Unique</th>
			<th>1,000 Unique</th>
			<th>100,000 Unique</th>
		</tr>
	</thead>
<tbody>
<tr align="right">
<td>10</td>	<td>30,443</td>	<td>35,229</td>	<td>30,068</td>
</tr>
<tr align="right">
<td>100</td>	<td>169,240</td>	<td>208,973</td>	<td>196,924</td>
</tr>
<tr align="right">
<td>1,000</td>	<td>1,442,566</td>	<td>1,466,887</td>	<td>1,624,803</td>
</tr>
<tr align="right">
<td>10,000</td>	<td>14,243,882</td>	<td>14,492,360</td>	<td>21,312,597</td>
</tr>
<tr align="right">
<td>100,000</td>	<td>143,051,464</td>	<td>141,408,320</td>	<td>192,644,214</td>
</tr>
<tr align="right">
<td>1,000,000</td>	<td>1,425,506,632</td>	<td>1,394,233,550</td>	<td>1,447,067,496</td>
</tr>
</tbody>
</table>

The revised implementation has better performance than the original one, but it still isn't as fast as the non-concurrent version.  It might scale better with different hardware and different tuple types though.

### Ordered Implementation

If we are willing to make some restrictions on how the input channel's data is ordered, then we can avoid using the map altogether.  Instead, we can just compare each input tuple with the previously sent result tuple, and if it is equal to it, then we discard it.

{% gist jonlawlor/b490a04d363a6f04276c %}
This has a big advantage over the first two in terms of memory use.  Also, it returns an ordered output.  However, it can't be used for unordered inputs.  Knowing the characteristics of the input makes a big difference in terms of memory, and as we will see, it also makes it faster.

### Ordered Performance

I haven't benchmarked the memory usage of the previous two, but by inspection we can see that the memory usage will grow with the number of unique tuples, without bound.  The ordered program has constant memory usage.  Here is its performance (ns/op):

<table>
	<thead>
		<tr align="right">
			<th>Tuples</th>
			<th>3 Unique</th>
			<th>1,000 Unique</th>
			<th>100,000 Unique</th>
		</tr>
	</thead>
<tbody>
<tr align="right">
<td>10</td>	<td>11,941</td>	<td>13,041</td>	<td>12,791</td>
</tr>
<tr align="right">
<td>100</td>	<td>89,945</td>	<td>101,685</td>	<td>101,925</td>
</tr>
<tr align="right">
<td>1,000</td>	<td>908,702</td>	<td>996,094</td>	<td>995,019</td>
</tr>
<tr align="right">
<td>10,000</td>	<td>8,780,944</td>	<td>10,039,774</td>	<td>10,078,654</td>
</tr>
<tr align="right">
<td>100,000</td>	<td>88,551,917</td>	<td>101,807,795</td>	<td>101,829,774</td>
</tr>
<tr align="right">
<td>1,000,000</td>	<td>884,714,522</td>	<td>1,022,781,810</td>	<td>1,015,324,417</td>
</tr>
</tbody>
</table>

Well, this is slightly slower than the unordered version (~20% compared to best) but while the unordered version is still building up its map, this version is faster.  As an added bonus, the values coming out will be in the same order as those coming in.

Here's the same table in ns/tuple:

<table>
	<thead>
		<tr align="right">
			<th>Tuples</th>
			<th>3 Unique</th>
			<th>1,000 Unique</th>
			<th>100,000 Unique</th>
		</tr>
	</thead>
<tbody>
<tr align="right">
<td>10</td>	<td>1,194</td>	<td>1,304</td>	<td>1,279</td>
</tr>
<tr align="right">
<td>100</td>	<td>899</td>	<td>1,017</td>	<td>1,019</td>
</tr>
<tr align="right">
<td>1,000</td>	<td>909</td>	<td>996</td>	<td>995</td>
</tr>
<tr align="right">
<td>10,000</td>	<td>878</td>	<td>1,004</td>	<td>1,008</td>
</tr>
<tr align="right">
<td>100,000</td>	<td>886</td>	<td>1,018</td>	<td>1,018</td>
</tr>
<tr align="right">
<td>1,000,000</td>	<td>885</td>	<td>1,023</td>	<td>1,015</td>
</tr>
</tbody>
</table>

So long term, this seems pretty stable around 1M tuples/s.  I'm not sure why there is a jump from 3 unique values to 1000 unique values, so if anyone has any ideas on that I'd like to hear them.

On the other hand, the ordered distinct requires that the input is sorted.  If it is not already sorted, then we either have to use a different algorithm, or we have to read the entire (possibly *enormous*) input channel, and sort the whole thing before we can start producing results, which will result in enormous latency.

### Conclusions
Even something as simple as finding unique values in an input channel exhibits some interesting tradeoffs, and we've barely scratched the surface of data streams.  As a general rule: unsorted data streams often (but not always) require an unbounded amount of memory for exact results.  If we can impose some restrictions on how tuples will be input to a data stream operation, then we can reduce the amount of memory used dramatically, although that doesn't always mean better performance.  And adding more cores doesn't always result in better performance.

We'll probably be revisiting these algorithms later, using the unsafe package.  If you have suggestions as to better implementations (or implementations that are better in some situations), I'll try to either update this post, or add them to the followup.
